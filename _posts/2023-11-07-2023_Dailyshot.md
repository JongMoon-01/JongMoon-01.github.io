---
title: "DailyShot"
date: 2023-12-29 02:23:00 +0900
categories: [Side Project]
tags: [ESG, TeamProject, Python, flask]
---
# 1. Motivation for Participation
2학년 2학기 당시 공모전을 나가면서 다른 학과 사람들하고 하는 프로젝트를 해야겠다는 생각이 들어 융합캡스톤디자인 수업을 듣게 되었다. 해당 수업을 진행하며 자연스레 타학과 학생들과 협업할 기회가 생겼고 4명으로 팀을 결성하여 웃음을 주제로 프로젝트를 진행하게 되었다.

# 2. Selection of Project Topic
나는 그때 당시 느꼈던 점이 아무래도 공과대학에서 프로젝트를 진행하다보니 프로젝트 내용에 인문이라는 단어는 찾을 수 없을만큼 이과감성만 남아있는것을 많이 느꼈다. 바로 이전에 했던 프로젝트 불법 주정차 신고도 진짜 문제를 파고들어보면 불법주정차를 할 수 밖에 없는 구역(특히, 주차장 부족)이 생각보다 많았고 만약 우리 시스템이 그대로 상용화 되었다면 많은 사람들은 본질적인 문제를 해결하지도 못한체 불법주정차 벌금이나 계속 내면서 스트레스를 받거나 자신이 사는 지역보다 멀리 떨어진 곳에 주차해 삶의 질이 떨어졌을 것이다. 그래서 이번 프로젝트는 인문학과 밀접한 관련이 있는 사람들과 진행을 하게 되었다.

<center>
<img src="https://github.com/cisco/openh264/assets/56510688/eac08801-86c1-4b88-a2fe-46d763d99acc" width="720" height=""/>
<p><b>[그림1]. DailyShot Keyword </b></p>
</center>


# 3. Project Process

## 3.1. Role Sharing
안타깝게도 4명중에 개발자가 나뿐이여서 개발담당을 맡게되었고 나머지는 기획과 홍보쪽으로 빠지게 되었습니다.

## 3.2. My Task
공간과 웃음 사진을 키워드를 조합해서 어떤 서비스를 구현할까 많이 고민했을 때 음성에 영향을 받는 포토존을 만들면 색다로운 경험이 되지 않을까 생각을 해 해당 서비스 개발을 하게 되었다.

```
스레딩 기능 : threading
이미지 캡처 및 처리 : OpenCV, moviepy
오디오 음성 처리 : Pyaudio, wave
서버 : Flask
```

<center>
<img src="https://github.com/cisco/openh264/assets/56510688/eac08801-86c1-4b88-a2fe-46d763d99acc" width="720" height=""/>
<p><b>[그림2]. 데이터 흐름 </b></p>
</center>



### 3.2.1. 초기설정

```python
# Flask 앱 및 OpenCV 설정
app = Flask(__name__, static_folder='static')  # Flask 앱 생성 및 정적 파일 폴더 설정
cap = cv2.VideoCapture(0)  # 웹캠을 통한 영상 캡처를 위한 VideoCapture 객체 생성
saved_images = []  # 캡처된 이미지를 저장할 리스트
timer_start_time = None  # 이미지 캡처 간격을 제어하기 위한 타이머 시작 시간
timer_duration = 5  # 이미지 캡처 간격 (예시로 5초로 설정)
check = 0  # 이미지 캡처 및 처리가 완료되었는지 확인하기 위한 변수

# OpenCV의 FFMPEG 캡처 옵션 설정
os.environ["OPENCV_FFMPEG_CAPTURE_OPTIONS"] = "video_codec;h264_cuvid,preset;slow"

# 사운드 녹음을 위한 PyAudio 설정
CHUNK = 1024  # 오디오를 읽을 데이터의 크기
FORMAT = pyaudio.paInt16  # 오디오 포맷 설정
CHANNELS = 1  # 오디오 채널 설정 (단일 채널)
RATE = 44100  # 샘플링 속도 설정
RECORD_SECONDS = 20  # 녹음 시간 (20초로 설정)
WAVE_OUTPUT_FILENAME = "output.wav"  # 녹음된 오디오를 저장할 파일 이름
CAPTURED_IMAGE_FILENAME = "static/captured_image.jpg"  # 캡처된 이미지 파일 이름
max_amplitude = 0  # 최대 진폭 초기화
current_amplitude = 0  # 현재 진폭 초기화
```

### 3.2.2. VideoCapture

```python
def generate_frames():
    global saved_images, timer_start_time, cap, check

    # check 변수가 1이 아닌 동안 계속해서 프레임 생성
    while check != 1:
        current_time = time.time()  # 현재 시간 측정
        if timer_start_time is None:
            timer_start_time = current_time  # 타이머 시작 시간 초기화
        ret, frame = cap.read()  # 웹캠에서 프레임 읽기
        frame_copy = frame.copy()  # 프레임 복사
        remaining_time = max(0, timer_duration - int(current_time - timer_start_time))  # 남은 시간 계산
        cv2.putText(frame_copy, f"Timer: {remaining_time}s", (10, frame.shape[0] - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)  # 프레임에 타이머 정보 텍스트 추가
        _, buffer = cv2.imencode('.jpg', frame_copy)  # 프레임을 JPEG 형식으로 인코딩
        frame_data = buffer.tobytes()  # 바이트로 변환된 프레임 데이터
        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + frame_data + b'\r\n')  # 프레임 데이터를 yield하여 클라이언트로 전송

        # 남은 시간이 0이 되면 이미지 캡처
        if remaining_time == 0:
            copy_image = frame.copy()  # 프레임 복사
            saved_image = cv2.resize(copy_image, (640, 480), cv2.INTER_AREA)  # 이미지 크기 조정
            saved_images.append(saved_image)  # 캡처된 이미지 저장
            print("이미지 저장됨!")
            timer_start_time = None  # 타이머 시작 시간 초기화
            print(len(saved_images))
            if len(saved_images) == 4:
                # 이미지들을 경계선과 함께 결합하여 하나의 이미지 생성
                padded_images = [cv2.copyMakeBorder(img, 30, 30, 30, 30, cv2.BORDER_CONSTANT, value=[0, 174, 64]) for
                                 img in saved_images]
                combined_images = np.vstack(padded_images)  # 이미지를 수직으로 결합
                black_background = np.zeros_like(combined_images)  # 결합된 이미지와 같은 크기의 검은 배경 생성
                black_background[:combined_images.shape[0], :combined_images.shape[1]] = combined_images  # 결합된 이미지를 검은 배경에 추가
                file_name = './ESG/static/photo_image.jpg'  # 이미지 파일 경로 지정
                cv2.imwrite(file_name, black_background)  # 이미지를 파일로 저장
                print(f"이미지가 {file_name}로 저장되었습니다.")
                saved_images = []  # 저장된 이미지 리스트 초기화
                check = 1  # check 변수를 1로 설정하여 이미지 캡처 완료를 알림

    # 이미지 경로 설정
    input_image_path = "./ESG/static/photo_image.jpg"
    output_video_path = "./ESG/static/frontground.mp4"
    create_video(input_image_path, output_video_path, frame_rate=30.0, duration=10)  # 동영상 생성
    check = 0  # check 변수 초기화
```

### 3.2.3. RecordAudio

```python
def record_sound():
    global current_amplitude, max_amplitude  # 전역 변수 사용 선언

    print("오디오 녹음 시작.")  # 오디오 녹음 시작 메시지 출력
    frames = []  # 녹음된 오디오 프레임을 저장할 리스트

    # 주어진 시간 동안 오디오 녹음
    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):
        data = stream.read(CHUNK)  # 오디오 데이터 읽기
        frames.append(data)  # 읽은 데이터를 프레임 리스트에 추가

        # 최대 진폭 추적
        audio_data = np.frombuffer(data, dtype=np.int16)  # 바이너리 데이터를 int16 형식의 넘파이 배열로 변환
        current_amplitude = np.max(np.abs(audio_data))  # 현재 진폭 계산
        max_amplitude = max(max_amplitude, current_amplitude)  # 최대 진폭 업데이트

        # 현재까지의 최대 진폭 출력
        # print("현재 최대 진폭:", max_amplitude)

        # 현재 진폭 출력
        # print("현재 최대 진폭:", current_amplitude)

    print("녹음 완료. 최종 최대 진폭:", max_amplitude)  # 녹음 완료 및 최종 최대 진폭 출력

    # 오디오 스트림 정지 및 닫기
    stream.stop_stream()
    stream.close()

    # PyAudio 객체 종료
    p.terminate()

    # 오디오 데이터를 WAV 파일로 저장
    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')  # WAV 파일을 바이너리 쓰기 모드로 열기
    wf.setnchannels(CHANNELS)  # 채널 수 설정
    wf.setsampwidth(p.get_sample_size(FORMAT))  # 샘플 폭 설정
    wf.setframerate(RATE)  # 샘플링 주파수 설정
    wf.writeframes(b''.join(frames))  # 프레임 데이터를 파일에 쓰기
    wf.close()  # WAV 파일 닫기
```

### 3.2.3. SoundResponsibleFrame

```python
def soundResponsibleFrame(cap, wf, output_video):
    print("이미지 캡처 및 동영상 저장 시작.")
    hue_factor = 0  # 색상 변화를 위한 hue_factor 초기화
    target_hue = 0  # 목표 색상 초기값 설정

    while True:
        ret, frame = cap.read()  # 웹캠에서 프레임 읽기

        # 현재 프레임의 오디오 진폭 측정
        data = wf.readframes(CHUNK)  # 오디오 프레임 읽기
        if not data:  # 더 이상 데이터가 없으면 종료
        break
        audio_data = np.frombuffer(data, dtype=np.int16)  # 바이너리 데이터를 int16 형식의 넘파이 배열로 변환
        current_amp = np.max(np.abs(audio_data))  # 현재 오디오 진폭 측정

        # 오디오 진폭에 따라 웹캠 필터 조절
        if current_amp > 32767 / 2:  # 오디오 진폭이 높은 경우
            # 노란색 ~ 빨간색의 색상 변화 설정
            target_hue = 25 + ((current_amp - 32767 / 2) / 32767.0) * 150  # 범위: 30부터 180까지
        else:  # 오디오 진폭이 낮은 경우
            # 하늘색 ~ 보라색의 색상 변화 설정
            target_hue = ((current_amp / (32767 / 2)) * 120)  # 범위: 0부터 120까지

        # hue_factor를 부드럽게 변화시키기
        hue_factor = 0.95 * target_hue + 0.05 * hue_factor

        # 블러 효과 적용
        blur_factor = 30 * (current_amp / 32767.0)  # 블러 강도 계산
        blur_factor = max(0, min(30, blur_factor))  # 최대 블러 강도 제한
        blurred_frame = cv2.GaussianBlur(frame, (int(blur_factor)*2+1, int(blur_factor)*2+1), 0)  # 가우시안 블러 적용

        # 색 변환
        hsv_frame = cv2.cvtColor(blurred_frame, cv2.COLOR_BGR2HSV)  # BGR 색상 공간을 HSV 색상 공간으로 변환
        hsv_frame[:, :, 0] = hue_factor  # hue 채널에 색상 변화 적용
        colored_frame = cv2.cvtColor(hsv_frame, cv2.COLOR_HSV2BGR)  # HSV 색상 공간을 BGR 색상 공간으로 다시 변환

        # 동영상에 프레임 추가
        output_video.write(colored_frame)  # 색상 변환된 프레임을 동영상에 추가

    print("이미지 캡처 및 동영상 저장 완료.")
```

### 3.2.4. ChromakeyComposition

```python
def chromakey_composition(foreground_path, background_path, output_path, lower_color, upper_color):
    # 정적 이미지와 동적 이미지 읽기
    dynamic1_image = cv2.VideoCapture(foreground_path)  # 전경
    dynamic2_image = cv2.VideoCapture(background_path)  # 후경

    # 동적 이미지의 프레임 속성 설정
    frame_rate = dynamic2_image.get(cv2.CAP_PROP_FPS)
    frame_duration = 1 / frame_rate

    # VideoWriter 객체 생성
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 코덱 설정
    output_width, output_height = 700, 2160  # 출력 동영상 크기
    out = cv2.VideoWriter(output_path, fourcc, frame_rate, (output_width, output_height))

    while True:
        # 동적 이미지에서 현재 프레임 읽어오기
        ret1, frame1 = dynamic1_image.read()
        ret2, frame2 = dynamic2_image.read()

        # 동적 이미지가 끝났을 경우 종료
        if not ret1 or not ret2:
            break

        # 동적 이미지 크기를 700x2160으로 변경
        frame_resized = cv2.resize(frame2, (700, 2160), interpolation=cv2.INTER_LINEAR)

        # 크로마키 합성을 위해 HSV 색공간으로 변환
        hsv = cv2.cvtColor(frame1, cv2.COLOR_BGR2HSV)

        # 크로마키 영역을 찾기 위한 마스크 생성
        mask = cv2.inRange(hsv, lower_color, upper_color)

        # 크로마키 합성 수행
        result = cv2.copyTo(frame_resized, mask, frame1)

        # 결과 동영상에 현재 프레임 추가
        out.write(result)

        # 합성된 이미지 출력 (주석 처리)
        # cv2.imshow('Chromakey Composition', result)

        # 'q' 키를 누를 경우 종료
        #if cv2.waitKey(1) & 0xFF == ord('q'):
        #    break

    # 사용한 리소스 해제
    dynamic1_image.release()
    dynamic2_image.release()
    out.release()
```

### 3.2.5. convert_to_h264

```python
def convert_to_h264(input_file, output_file):
    # 비디오 클립을 로드합니다.
    video_clip = VideoFileClip(input_file)

    # H.264 코덱으로 비디오를 변환합니다.
    h264_clip = video_clip.write_videofile(output_file, codec='libx264', audio_codec='aac')

    # 메모리 해제
    #h264_clip.close()
    video_clip.close()
```

### 3.2.6. Flask

```python
# '/' 경로에 대한 GET 및 POST 메서드 처리
@app.route('/', methods=['GET', 'POST'])
def index():
    return render_template('index.html')  # index.html 페이지 렌더링

# '/capture' 경로에 대한 GET 메서드 처리
@app.route('/capture')
def capture():
    global phone_number, name
    if request.method == 'GET':  # GET 요청이면
        phone_number = request.args.get('phone_number')  # 전화번호 가져오기
        name = request.args.get('name')  # 이름 가져오기
    if check == 1:  # check가 1이면 (이미 처리가 완료되었으면)
        print("redirect")
        return redirect(url_for('result'))  # 'result' 라우트로 리다이렉트
    return render_template('index_capture.html')  # index_capture.html 페이지 렌더링

# '/video_feed' 경로에 대한 GET 메서드 처리
@app.route('/video_feed')
def video_feed():
    sound_thread.start()  # 사운드 스레드 시작
    image_thread.start()  # 이미지 스레드 시작
    return Response(generate_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')  # generate_frames() 함수 실행 결과 반환

# '/check_result' 경로에 대한 GET 메서드 처리
@app.route('/check_result')
def check_result():
    return str(check)  # check 값 반환

output_video_paths = ['background1.mp4', 'background2.mp4', 'background3.mp4', 'background4.mp4']

# '/result' 경로에 대한 GET 메서드 처리
@app.route('/result')
def result():
    return render_template('result.html', output_video_paths=output_video_paths)  # result.html 페이지 렌더링

# '/chromakey' 경로에 대한 POST 메서드 처리
@app.route('/chromakey', methods=['POST'])
def chromakey():
    if request.method == 'POST':  # POST 요청이면
        lower_color = np.array([35, 100, 100])  # 수정된 값
        upper_color = np.array([85, 255, 255])  # 수정된 값

        index = request.form['index']  # 인덱스 가져오기
        foreground_path = './ESG/static/frontground.mp4'  # 전경 동영상 경로
        background_path = f'./ESG/static/background{index}.mp4'  # 선택된 배경 동영상 경로
        output_directory = f'./ESG/static/{phone_number}/'  # 출력 디렉토리 경로

        output_filename = f'{name}_{index}.mp4'  # 출력 파일 이름
        output_path = os.path.join(output_directory, output_filename)  # 출력 파일 경로

        # 비디오 캡처 매개변수 설정
        cap_audio = cv2.VideoCapture(background_path)
        cap_audio.set(cv2.CAP_PROP_FRAME_WIDTH, 700)
        cap_audio.set(cv2.CAP_PROP_FRAME_HEIGHT, 2160)

        # 음성 파일 경로 설정
        AUDIO_FILE_PATH = "output.wav"

        # 동영상 저장 매개변수 설정
        OUTPUT_FILENAME = "audio_frame.mp4"
        OUTPUT_VIDEO_FILENAME = os.path.join(output_directory, OUTPUT_FILENAME)
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        OUTPUT_VIDEO = cv2.VideoWriter(OUTPUT_VIDEO_FILENAME, fourcc, 30 * 0.8, (700, 2160))

        wf = wave.open(AUDIO_FILE_PATH, 'rb')  # 음성 파일 열기
        soundResponsibleFrame(cap_audio, wf, OUTPUT_VIDEO)  # 함수 호출
        OUTPUT_VIDEO.release()
        cap_audio.release()

        # Chromakey 합성 수행
        chromakey_composition(foreground_path, OUTPUT_VIDEO_FILENAME, output_path, lower_color, upper_color)

        return redirect(url_for('chromakey_result', index=index))  # 'chromakey_result' 라우트로 리다이렉트

# '/chromakey_result/<int:index>' 경로에 대한 GET 메서드 처리
@app.route('/chromakey_result/<int:index>', methods=['GET'])
def chromakey_result(index):
    if request.method == 'POST':  # POST 요청이면
        cap.release()
        return render_template('index.html')  # index.html 페이지 렌더링

    input_directory = f'./ESG/static/{phone_number}/'  # 입력 디렉토리 경로
    input_filename = f'{name}_{index}.mp4'  # 입력 파일 이름
    input_path = os.path.join(input_directory, input_filename)  # 입력 파일 경로

    output_filename = f'{name}_{index}H264.mp4'  # 출력 파일 이름
    output_path = os.path.join(input_directory, output_filename)  # 출력 파일 경로

    render_directory = f'/{phone_number}/'  # 렌더링 디렉토리 경로
    render_path = os.path.join(render_directory, output_filename)  # 렌더링 파일 경로

    # H264 포맷으로 변환
    convert_to_h264(input_path, output_path)

    return render_template('chromakey_result.html', output_path=render_path)  # chromakey_result.html 페이지 렌더링

# Flask 애플리케이션 실행
if __name__ == '__main__':
    sound_thread = threading.Thread(target=record_sound)  # 사운드 스레드 생성
    image_thread = threading.Thread(target=generate_frames)  # 이미지 스레드 생성
    app.run(debug=True)  # Flask 애플리케이션 실행 (디버그 모드 활성화)
```

# 4. result
## 4.1. index.html

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Index</title>
    <style>
        body {
            display: flex;
            align-items: center;
            justify-content: center;
            height: 100vh;
            margin: 0;
        }

        .container {
            text-align: center;
            transform: scale(1.5);
        }

        form {
            max-width: 600px; /* 400px * 1.5 = 600px */
            width: 100%;
            padding: 30px; /* 20px * 1.5 = 30px */
            border: 1px solid #ccc;
            border-radius: 12px; /* 8px * 1.5 = 12px */
            box-shadow: 0 0 15px rgba(0, 0, 0, 0.1);
            display: inline-block;
            text-align: left;
        }

        label {
            display: block;
            margin-bottom: 12px; /* 8px * 1.5 = 12px */
        }

        input {
            width: 100%;
            padding: 12px; /* 8px * 1.5 = 12px */
            margin-bottom: 18px; /* 12px * 1.5 = 18px */
            box-sizing: border-box;
        }

        input[type="submit"] {
            background-color: #4caf50;
            color: white;
            cursor: pointer;
        }

        input[type="submit"]:hover {
            background-color: #45a049;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>안녕하세요! DaliyShot입니다!</h1>

        <form action="/capture" method="get">
            <label for="phone_number">Phone Number:</label>
            <input type="text" id="phone_number" name="phone_number" required>
            <br>
            <label for="name">Name:</label>
            <input type="text" id="name" name="name" required>
            <br>
            <input type="submit" value="Confirm">
        </form>
    </div>
</body>
</html>
```

<center>
<img src="https://github.com/cisco/openh264/assets/56510688/e04e6e82-b225-4f9e-a342-e66f66ab141b" width="720" height=""/>
<p><b>[그림1]. 초기화면</b></p>
</center>

## 4.2. VideoCapture.html
실제 실행화면은 opencv에서 read하고있는걸 그대로 프론트에서 출력하고 왼쪽 하단에 타이머 표시하는게 끝입니다.

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Result Page</title>
    <style>
        body {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            margin: 0;
        }

        .video-container {
            display: flex;
            justify-content: space-around;
            margin-top: 20px; /* 위 여백 조절 */
        }

        video {
            margin-bottom: 10px; /* 각 비디오 아래 여백 추가 */
            margin-right: 50px;
        }

        form {
            margin-top: 10px; /* 폼 위 여백 조절 */
            display: flex;
            flex-direction: column; /* 버튼 및 텍스트 수직 정렬 */
            align-items: center; /* 아이템 가로 중앙 정렬 */
        }

        button {
            width: 100px; /* 버튼 너비 설정 */
            margin-top: 10px; /* 버튼 위 여백 추가 */
        }
    </style>
</head>
<body>
    <h1>Result Page</h1>

    <div class="video-container">
        {% for video_path in output_video_paths %}
            <div>
                <video loop="" autoplay="autoplay" muted="muted" width="700" height="2160">
                    <source src="{{ url_for('static', filename=video_path) }}" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <form action="{{ url_for('chromakey') }}" method="post">
                    <input type="hidden" name="index" value="{{ loop.index }}">
                    <button type="submit">Chromakey {{ loop.index }}</button>
                </form>
            </div>
        {% endfor %}
    </div>

    <script>
        function startChromakey(index) {
            // 버튼 클릭 시 Flask 애플리케이션의 /chromakey 엔드포인트 호출
            fetch(`/chromakey/${index}`)
                .then(response => response.text())
                .then(data => console.log(data))
                .catch(error => console.error('Error:', error));
        }
    </script>
</body>
</html>
```

<center>
<img src="https://github.com/cisco/openh264/assets/56510688/ca3b563d-4cce-47f9-be27-4417a2fa3c09" width="720" height=""/>
<p><b>[그림2]. Videocapture.html</b></p>
</center>

## 4.3. SelectedBackground.html

```html
<!-- chromakey_result.html -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chromakey Result</title>
    <style>
        body {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            margin: 0;
        }

        video {
            margin-top: 20px; /* 위 여백 조절 */
        }

        #result-text {
            margin-top: 20px; /* 결과 텍스트 위 여백 조절 */
        }

        #download-link {
            margin-top: 10px; /* 다운로드 링크 위 여백 조절 */
        }

        #confirm-button {
            margin-top: 20px; /* 확인 버튼 위 여백 조절 */
        }
    </style>
</head>
<body>
    <h1>Chromakey Result</h1>

    <video loop="" autoplay="autoplay" width="700" height="2160">
        <source src="{{ url_for('static', filename=output_path) }}" type="video/mp4">
        Your browser does not support the video tag.
    </video>

    <p id="download-link">문자를 통해 다운로드 링크를 보내드리겠습니다</p>

    <form action="{{ url_for('index') }}" method="get">
        <button id="confirm-button" type="submit">확인</button>
    </form>
</body>
</html>
```

## 4.4. 최종 결과

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Result Page</title>
    <style>
        body {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            margin: 0;
        }

        .video-container {
            display: flex;
            justify-content: space-around;
            margin-top: 20px; /* 위 여백 조절 */
        }

        video {
            margin-bottom: 10px; /* 각 비디오 아래 여백 추가 */
            margin-right: 50px;
        }

        form {
            margin-top: 10px; /* 폼 위 여백 조절 */
            display: flex;
            flex-direction: column; /* 버튼 및 텍스트 수직 정렬 */
            align-items: center; /* 아이템 가로 중앙 정렬 */
        }

        button {
            width: 100px; /* 버튼 너비 설정 */
            margin-top: 10px; /* 버튼 위 여백 추가 */
        }
    </style>
</head>
<body>
    <h1>Result Page</h1>

    <div class="video-container">
        {% for video_path in output_video_paths %}
            <div>
                <video loop="" autoplay="autoplay" muted="muted" width="700" height="2160">
                    <source src="{{ url_for('static', filename=video_path) }}" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <form action="{{ url_for('chromakey') }}" method="post">
                    <input type="hidden" name="index" value="{{ loop.index }}">
                    <button type="submit">Chromakey {{ loop.index }}</button>
                </form>
            </div>
        {% endfor %}
    </div>

    <script>
        function startChromakey(index) {
            // 버튼 클릭 시 Flask 애플리케이션의 /chromakey 엔드포인트 호출
            fetch(`/chromakey/${index}`)
                .then(response => response.text())
                .then(data => console.log(data))
                .catch(error => console.error('Error:', error));
        }
    </script>
</body>
</html>
```

<center>
<video width="480" height="1080" controls>
  <source src="https://github.com/cisco/openh264/assets/56510688/8b53d5dd-03bb-43d0-9ec6-184d94cf25bf" type="video/mp4">
  Your browser does not support the video tag.
</video>
<p><b>[영상1]. result.html</b></p>
</center>