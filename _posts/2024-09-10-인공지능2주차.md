---
title: "인공지능 2주차"
date: 2024-09-10 02:23:00 +0900
categories: [AI]
tags: [AI, Network, Python, ML, Tensorflow]
---

# 1. 머신러닝과 기초수학

- 벡터의 정의
    - 물리 : 벡터는 크기와 방향을 가진 값이다.
    - 벡터는 값의 모임이며 벡터를 구성하는 값에는 순서가 존재

- 유클리드 공간의 벡터는 열 또는 행으로 표현된다.

* 벡터 x에 대하여 x^T 전치(Transpose) 연산의 결과물로 전치 연산은 열벡터를 행벡터 또는 그반대로 변환한다.

- n차원 공간과 n-벡터
    - n개의 성분이 있는 일반 형식의 벡터("n 벡터"라 호칭)은 n차원 공간 R^n에 부분집합 x에 존재한다.(R은 실수 집합 의미)

```
벡터의 표현
(x_1, x_2, x_3)
<x_1, x_2, x_3>
    [x_1]
x = [x_2] = [x_1, x_2, x_3]^T
    [x_3]
x = [x_1, x_2, x_3]
```

벡터를 공간의 점으로 기하학적으로 해석하면, 머신러닝에서 다루는 데이터를 공간의 점 집합으로 간주할 수 있다.
    - 벡터를 공간의 점으로 간주하면 데이터를 다양한 방식으로 시각화하여 데이터의 특성을 직관적으로 파악하는데 도움이된다.(보통 전처리 과정에서 진행된다.)

<center>
<img src="https://github.com/user-attachments/assets/329534a4-1643-43f5-890d-6924d2f3ba64" width="720" height=""/>
<p><b>[그림1]. 벡터 연산</b></p>
</center>


## 1.1. 벡터의 Norm
- 벡터 크기는 Norm이라는 Sclar값으로 표현된다.(벡터의 크기 척도)
- Norm f는 다음 조건을 충족한다.
    - Scaling : f(ax) = |a|f(x)
    - Triangle inequality : f(x+y) <= f(x) + f(y)
    - 양의 함수 f(x) > 0

- 벡터 x의 norm L_p는 다음과 같다.

```
||x|| = (n M i=1 |x_i|^p)^1/p, x = [x_1, ... , x_n]^T

M은 시그마 
```

- 일반적으로 p = 1 ~ 무한대가 일반적인 Norm으로 사용된다.
- 수학에서 ||x||_∞  벡터 𝑥의 무한 노름 또는 최대 노름을 의미합니다. 이는 벡터의 원소 중 절댓값이 가장 큰 값을 취하는 노름입니다.

## 1.2. 벡터의 내적과 시잇각
- 벡터 내적 u • v = u^Tv = n M i=1 u_i*v_i = ||u||||v||cos(θ)
    - u • v = <u,v>
    - ||u||는 ||x||_p에서 p=2인 경우
    - θ = u와 v의 사잇각

- 내적연산은 대칭연산이다.

- 내적의 기하학적 해석

```

cos(θ) = u • v / ||u||||v||

θ = cos^-1(u • v / ||u||||v||)

```

## 1.3. hyperplanes
- 초평면은 차원이 주변공간보다 하나 작은 부분 공간이다. 국소공간이자 공간의 부분집합인듯
    - 이차원 공간에서 초평면은 직선이다.
    - n차원 벡터공간에서 초평면은 n-1차원을 가지며 공간을 두개의 절반 공간으로 나눈다.
- 머신러닝의 초평면은 Classification에 사용되는 decision boundary다.
    - 선형분류에서는 초평면의 양쪽에 있는 데이터가 서로 다른 실제값(Class)에 속한다고 해석

<center>
<img src="https://github.com/user-attachments/assets/8c25a35e-c67f-40a9-bee1-1e3aab17eb2c" width="720" height=""/>
<p><b>[그림2]. 단위 벡터</b></p>
</center>

# 1.4. 행렬
행렬(Matrix)은 데이터를 직사각형 배열로 나타낸 것으로, \( m \)개의 가로행(row)과 \( n \)개의 열(column)로 배열됩니다.  
행렬 \( A \)에서 \( i \)번째 행과 \( j \)번째 열에 해당하는 요소는 \( a_{ij} \)로 표현됩니다.

예시:
\[
A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix}
\]

# 1.5. 선형 종속 및 선형 독립
- **선형 종속**: 벡터들의 집합에서 어떤 벡터가 다른 벡터들의 선형 결합으로 표현될 수 있는 경우, 그 벡터들은 선형 종속입니다.
- **선형 독립**: 벡터들이 서로 독립적이며, 어떤 벡터도 다른 벡터들의 선형 결합으로 표현될 수 없는 경우를 선형 독립이라고 합니다.

# 1.6. 텐서
텐서(Tensor)는 스칼라(scalar), 벡터(vector), 행렬(matrix)의 일반화된 형태로, 다차원 배열을 의미합니다. 예를 들어, RGB 이미지의 경우 3차원 텐서로 나타낼 수 있으며, 각 차원은 가로, 세로, 채널을 나타냅니다.

---

# 2. 미분학
미분학(Differential Calculus)은 함수의 변화율을 연구하는 학문입니다.  
미분은 함수의 기울기를 나타내며, 다양한 함수(지수, 로그, 거듭제곱, 상수값 등)의 미분 공식을 통해 변화율을 계산할 수 있습니다.

## 2.1. 고계 도함수
고계 도함수(Higher-Order Derivatives)는 함수의 미분을 반복해서 수행한 도함수입니다.
- 1계 도함수: \( f'(x) \)
- 2계 도함수: \( f''(x) \)
- 3계 도함수: \( f'''(x) \), ...

## 2.2. 편미분
편미분(Partial Derivatives)은 여러 변수를 가진 함수에서 특정 변수에 대한 변화율을 구하는 방법입니다.  
예를 들어, 함수 \( f(x, y) \)에서 \( x \)에 대한 편미분은 \( \frac{\partial f}{\partial x} \)로 나타냅니다.

## 2.3. 기울기
기울기(Gradient)는 다변수 함수에서 각 변수에 대한 편미분을 벡터로 나타낸 것입니다.  
함수 \( f(x, y) \)의 기울기는 다음과 같이 표현됩니다:
\[
\nabla f = \left( \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right)
\]
기울기는 함수의 최댓값 또는 최솟값을 찾는 데 중요한 역할을 합니다.

## 2.4. 최적화
최적화(Optimization)는 주어진 함수에서 최대값이나 최소값을 찾는 과정입니다.  
- **경사하강법(Gradient Descent)**: 기울기를 이용해 함수의 최솟값을 찾는 방법입니다. 기울기의 반대 방향으로 이동하면서 최소점을 찾습니다.

## 2.5. 극소점
극소점(Local Minimum)은 함수가 특정 구간 내에서 최소값을 가지는 점입니다.  
- **전역최소값(Global Minimum)**: 함수 전체에서의 최소값.
- **지역최소값(Local Minimum)**: 특정 구간에서의 최소값.

## 2.6. 임계점
임계점(Critical Point)은 도함수가 0이 되는 점으로, 함수가 최대값이나 최소값을 가질 수 있는 지점입니다.  
즉, \( f'(x) = 0 \) 또는 정의되지 않는 지점을 임계점이라고 합니다.

---

# 2.7. 확률과 정수
확률(Probability)은 불확실한 사건이 발생할 가능성을 수치화한 것입니다.  
확률은 0에서 1 사이의 값을 가지며, 0은 불가능, 1은 확실함을 나타냅니다.

## 확률의 공리 (Kolmogorov의 세 공리)
1. 어떤 사건 \( A \)에 대해, 그 사건의 확률 \( P(A) \)는 \( 0 \leq P(A) \leq 1 \)의 범위에 있습니다.
2. 표본 공간 전체 사건의 확률은 1입니다. 즉, \( P(S) = 1 \).
3. 서로 배반인 사건 \( A \)와 \( B \)에 대해, \( P(A \cup B) = P(A) + P(B) \).

## 2.8. 베이즈 정리
베이즈 정리(Bayes' Theorem)는 조건부 확률을 계산하는 데 사용되는 정리로, 사전 확률과 데이터에 기반하여 사후 확률을 계산합니다.
\[
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\]
여기서:
- \( P(A|B) \): 사건 \( B \)가 일어났을 때 사건 \( A \)가 일어날 확률(사후 확률)
- \( P(B|A) \): 사건 \( A \)가 일어났을 때 사건 \( B \)가 일어날 확률
- \( P(A) \): 사건 \( A \)가 일어날 확률(사전 확률)
- \( P(B) \): 사건 \( B \)가 일어날 확률

## 2.9. 분산
분산(Variance)은 데이터가 평균에서 얼마나 떨어져 있는지를 나타내는 값입니다.  
데이터 \( X \)의 분산은 다음과 같이 계산됩니다:
\[
\text{Var}(X) = \mathbb{E}[(X - \mu)^2] = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2
\]
여기서 \( \mu \)는 평균값이고, \( x_i \)는 각 데이터 포인트입니다.

## 2.10. 확률 분포
확률 분포(Probability Distribution)는 확률 변수가 취할 수 있는 값들과 그 값들이 일어날 확률을 나타내는 함수입니다.  
- **이산 확률 분포(Discrete Probability Distribution)**: 확률 변수가 이산적인 값을 취할 때.
- **연속 확률 분포(Continuous Probability Distribution)**: 확률 변수가 연속적인 값을 취할 때.

대표적인 확률 분포로는 **정규분포(Normal Distribution)**, **이항분포(Binomial Distribution)**, **포아송 분포(Poisson Distribution)** 등이 있습니다.
