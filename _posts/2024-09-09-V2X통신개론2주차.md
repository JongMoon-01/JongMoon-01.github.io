---
title: "V2X 통신개론 2주차"
date: 2024-09-01 02:23:00 +0900
categories: [V2X통신개론]
tags: [V2X, Network, Python]
---

학습 목표
- 스트리밍 기본 원리
- ABS(Adative Bitrate Streaming) - MPEG-DASH(Dynamic Adative Streaming Over HTTP)
- ABS가 가지고 있는 Latency Problem에 대한 이해
- 저지연이 왜 V2X통신에서 중요한지

# 1. Motivations

## 1.1. Global Internet Growth and Trends

CISCO 조사결과 인터넷 트래픽의 가장 많은 사용량은 동영상 스트리밍 서비스에 소모하는것이다. 
ex. 유튭, 넷플릭스, 지포스 나우, tv+, STADIA, 프라임 비디오

<center>
<img src="https://github.com/user-attachments/assets/50c7a0bd-637d-4a8a-aa6b-bc361733333e" width="720" height=""/>
<p><b>[그림1]. 트래픽 소모량 </b></p>
</center>

<center>
<img src="https://github.com/user-attachments/assets/79babff1-74c1-4ed1-82a9-8014b68423bc" width="720" height=""/>
<p><b>[그림2]. STADIA Service </b></p>
</center>



## 1.2. What is Streaming?


1. 소스 (Source)
전송하려는 정보(데이터)를 말합니다. 예를 들어, 텍스트, 음성, 영상 등이 소스가 될 수 있습니다.

2. 소스 코딩 (Source Coding)
소스를 효율적으로 압축하여 데이터를 줄이는 과정입니다. 정보의 중복을 제거하거나 효율적인 표현 방식을 사용합니다. 예: 텍스트 압축, MP3 압축.

3. 채널 코딩 (Channel Coding)
데이터 전송 중 발생할 수 있는 오류를 검출하고 수정할 수 있도록 데이터를 추가 처리하는 과정입니다. 오류 정정을 위한 코드를 추가합니다. 예: 해밍 코드, 리드 솔로몬 코드.

4. 모듈레이션 (Modulation)
데이터를 전송하기 위해 전송 매체(전파, 광섬유 등)에 맞게 신호를 변환하는 과정입니다. 예: 진폭 변조(AM), 주파수 변조(FM).

<center>
<img src="https://github.com/user-attachments/assets/7d7346d3-92ca-4b1e-afe3-7c3817c3b432" width="720" height=""/>
<p><b>[그림3]. 옛날 동영상 시청 방식 </b></p>
</center>

각각의 비디오 부분을 독립적으로 받아서 실행될 수 있다.

<center>
<img src="https://github.com/user-attachments/assets/bbf2ca9c-01d6-4117-8228-c98575e1cab6" width="720" height=""/>
<p><b>[그림4]. video streaming service </b></p>
</center>

<center>
<img src="https://github.com/user-attachments/assets/3b8fc402-9d95-40bd-bb59-e1d19cf24e19" width="720" height=""/>
<p><b>[그림5]. 유튜브 video streaming service </b></p>
</center>

# 멀티플렉싱: 멀티미디어 관점 vs 전송 과정

## 1. 멀티미디어 관점의 멀티플렉싱
멀티미디어 관점에서 멀티플렉싱은 **여러 가지 미디어 데이터**(오디오, 비디오, 자막 등)를 하나의 **스트림**으로 결합하는 과정입니다. 이를 통해 다양한 미디어 데이터를 동기화하여 **효율적으로 전송**하고, 수신자는 이를 동시에 재생할 수 있습니다.

### 예시
- **비디오 스트리밍**: 비디오와 오디오 데이터를 하나의 전송 스트림으로 결합하여 사용자가 동시에 시청하고 청취할 수 있게 합니다.
- **멀티미디어 파일 형식**: MP4, MKV와 같은 컨테이너 형식은 여러 미디어 트랙(비디오, 오디오, 자막 등)을 하나의 파일로 결합합니다. 이를 멀티플렉싱 방식으로 처리해 동시 재생이 가능합니다.

### 특징
- 여러 미디어 형식(오디오, 비디오, 자막 등)을 **동기화**하여 전송
- 멀티미디어 파일 형식에서 **효율적인 저장 및 재생** 가능
- 오디오와 비디오를 하나의 트랙으로 결합해 **동시에 처리**할 수 있음

---

## 2. 전송 과정에서의 멀티플렉싱
전송 과정에서의 멀티플렉싱은 **여러 데이터 신호를 하나의 전송 매체**(채널)로 결합하여 전송하는 방식입니다. 이를 통해 **다양한 데이터 흐름**을 하나의 채널로 전송하여 **효율성을 극대화**할 수 있습니다.

### 주요 방식
1. **시분할 다중화 (TDM, Time Division Multiplexing)**
   - 시간 단위로 **각 데이터를 순차적으로 전송**하는 방식입니다. 여러 데이터 흐름이 하나의 채널을 **시간 슬롯**으로 나눠서 사용합니다.
   - **예시**: 전화 시스템에서 여러 통화를 시분할로 처리

2. **주파수분할 다중화 (FDM, Frequency Division Multiplexing)**
   - **서로 다른 주파수 대역**을 사용해 여러 신호를 동시에 전송하는 방식입니다. 각 신호는 충돌 없이 **별도 주파수 대역**을 차지합니다.
   - **예시**: 라디오 방송에서 각 방송국이 서로 다른 주파수를 사용

3. **코드분할 다중화 (CDM, Code Division Multiplexing)**
   - 각 신호에 **고유한 코드**를 부여하여 같은 주파수 대역에서 데이터를 동시에 전송하는 방식입니다. 수신자는 자신에게 할당된 코드로 데이터를 추출합니다.
   - **예시**: 무선 통신(WCDMA)에서 여러 사용자에게 고유 코드를 할당해 통신

### 특징
- **여러 데이터 흐름**을 하나의 전송 채널로 결합해 **대역폭 효율**을 극대화
- 전송 매체의 **용량을 최대한 활용** 가능
- **전송 효율**을 높이기 위해 시간, 주파수, 코드 등을 사용해 데이터를 구분

---

## 요약 비교
- **멀티미디어 관점의 멀티플렉싱**: 여러 미디어 데이터를 동기화하고 하나의 스트림으로 결합해 **효율적인 재생**에 중점.
- **전송 과정에서의 멀티플렉싱**: 여러 데이터 신호를 하나의 물리적 채널로 결합해 **효율적으로 전송**하는 데 초점.

<center>
<img src="https://github.com/user-attachments/assets/2784987c-88af-42e9-88f8-a5a6aea3c11f" width="720" height=""/>
<p><b>[그림6]. Streaming MainFactors </b></p>
</center>

오리지널 영상 + 오디오 -> 코덱(encoding) -> + META DATA(Subtitle, Resolution, Size, etc..) -> Video Multiplexing -> Streaming Server에 저장 -> 코덱(Decoding) -> Client video play

<center>
<img src="https://github.com/user-attachments/assets/4bed5f3a-8034-41ab-a419-1d5f95157f8f" width="720" height=""/>
<p><b>[그림7]. Streaming Flow </b></p>
</center>

## 1.3. Further Basic Information on Codec - Video

- H.264 / AVC
    - Video Codec 광범위함.
    - First codec using Group of pictures(GOP)

- H.265 / HEVC(High efficient Video codec)
    - Half the bitrate of H.264/AVC(약 절반정도 이상의 압축 효율)
    - Good for high resolution and live streaming
    - better image quailty
    - High resources to encode

- VP9(Royalty-free/Open-source)
    - More consistent and reliable
    - Currently using for Youtube
    - More difficult to encode and not widely supported

## 1.4. Further Basic Information on Codec - Audio

- Mp3(MPEG-2 Audio Layer 3)
    - Popular with wide support
    - Reduce data volume without noticeable quailty loss
    - Limited functionailty

- AAC(Advanced Audio Coding)
    - Widely Supported
    - More efficient than MP3
    - High Resources to encode

- AC-3(Dolby Digital Audio Codec 3)
    - 5.1 surround sound
    - Narrow device support 

## 1.5. Further Basic Information on Container

모듈화 처럼 각각 비디오 코덱과 오디오 코덱을 선택지에서 골라서 컨테이너 제작 가능

<center>
<img src="https://github.com/user-attachments/assets/ce86601f-abc3-4ebe-826a-04c8f6d7f7eb" width="720" height=""/>
<p><b>[그림7]. MP4 Container </b></p>
</center>

## 1.6. Further Basic Information on Transport

- TCP-based Protocol(e.g. RTMP, Adobe에서 만듦)
    - It is traditional and proven streaming technology using Flash
    - Designed to maintain persistent, low-latency connections
    - Does not have native support in iOS anad HTML5 그리고 Flash 지원 끝남
- HTTP-based Protocol(e.g. HLS,HTTP Live Streaming and DASH,Dynamic Adaptive Streaming over HTTP)
    - Rely on regular web servers to optimize the viewing experience
    - The potentiality to deliver Adaptive Bitrate Streaming (ABS)
    - Latency which is relatively high when compared to RTMP

## 요약 비교

|      **특징**      | **HLS**                              | **DASH**                               | **MPEG-DASH**                          |
|:------------------:|:------------------------------------:|:--------------------------------------:|:--------------------------------------:|
|    **표준 여부**   | Apple 독점                          | 오픈 표준                              | DASH 표준 기반 구현                    |
| **전송 프로토콜**  | HTTP 기반                           | HTTP 기반                              | HTTP 기반                              |
| **세그먼트 포맷**  | TS                                  | 자유(주로 MP4)                         | 자유(주로 MP4)                         |
| **Manifest 파일**  | M3U8                                | MPD                                    | MPD                                    |
| **적응형 스트리밍(ABS)**| 지원                                | 지원                                   | 지원                                   |
| **지원 플랫폼**    | 주로 iOS 및 macOS                   | 다수의 플랫폼 및 기기에서 지원         | 대부분의 플랫폼에서 사용               |
| **DRM 지원**       | AES 기반의 DRM                      | 다양한 DRM 시스템 지원                 | 다양한 DRM 시스템 지원                 |

# 2. Adptive Bitrate Streaming

FPS안에서 위치에 해당하는 픽셀값의 변화가 거의 없는 부분을 압축해서 보내는 것.

* Predictive-coded : 원본끼리 픽셀값의 변화가 있는 부분만 예측해서 값을 채워넣고 압축
* Bipredictive-coded : 원본과 predictive-coded간의 픽셀값의 변화가 있는 부분을 예측해서 값을 채워넣고 압축
* Intra-coeded(원본) : 독자적인 디코딩이 가능

<center>
<img src="https://github.com/user-attachments/assets/8e02bca1-0915-4d23-a49c-b45d826c33d2" width="720" height=""/>
<p><b>[그림8]. GOP </b></p>
</center>

1. Case 1 
    - Streaming Service에서 아무 시간대나 찍으면 그 시간대에 맞는 Iframe을 받아옴

2. Case 2
    - 보고 싶은 후반부분을 클릭해도 Plaiable region이 아니기 때문에 재생 불가

<center>
<img src="https://github.com/user-attachments/assets/35aebadf-d672-443a-b6fd-a35f4edd926a" width="720" height=""/>
<p><b>[그림9]. GOP and Non-GOP Case </b></p>
</center>

<center>
<img src="https://github.com/user-attachments/assets/5d07b8e4-9c42-46a3-b7ac-8a1020e7bd31" width="720" height=""/>
<p><b>[그림10]. GOP = Random access unit </b></p>
</center>

## 2.1. ABS - Basic Concept

채널 상황에 따라서 제공하는 Bit rate와 resolution이 다름. 예를 들면 정적이지 않고 무선인 스마트폰은 낮은 화질, 정적이고 유선인 데스크탑은 높은 화질의 영상을 제공

<center>
<img src="https://github.com/user-attachments/assets/2050607b-aa10-42bc-b42b-1f78c695aeb8" width="720" height=""/>
<p><b>[그림11-1]. ABS Concept 1 </b></p>
</center>

움직이는 모빌리티 안에서도 지속적으로 끊임없이 스트리밍을 받기 위해서 인터넷 상황에 따라 제공받는 서비스의 품질을 적절하게 제공함.

<center>
<img src="https://github.com/user-attachments/assets/4b0809bf-551b-40f2-9fe8-67a65c9e8814" width="720" height=""/>
<p><b>[그림11-2]. ABS Concept 2 </b></p>
</center>

## 2.2. DASH vs HLS

<center>
<img src="https://github.com/user-attachments/assets/d380746d-9962-4287-9347-e6844a2fa965" width="720" height=""/>
<p><b>[그림12]. DASH vs HLS </b></p>
</center>

## 2.3. MPEG DASH - E2E system

과정
1. HTML에 비디오 스트리밍 요청
2. 클라이언트에게 MPD 반환
3. Client의 Channel State와 MPD에 맞춰 적절한 화질의 영상 수준을 해석
4. 해석한 내용을 Server에 요청
5. 결과 반환

* MPD : Media Presentation Description(META DATA)

<center>
<img src="https://github.com/user-attachments/assets/99e544aa-dfae-42cd-b4d6-ee8be8e25b90" width="720" height=""/>
<p><b>[그림13]. DASH Architecture </b></p>
</center>

시나리오를 살펴보면 Client의 상황에 따라 적절한 bitrate를 동적으로 제공하는 것을 확인할 수 있다.

<center>
<img src="https://github.com/user-attachments/assets/129384d2-464a-407b-9bb3-fdd6c3f9f184" width="720" height=""/>
<p><b>[그림14]. DASH System </b></p>
</center>

## 2.4. MPEG DASH - MPD

MPD는 크게 Period, Adaptation, Segment로 구분된다.

Period는 간단하게 동영상을 단위 시간당으로 구분하여 자른것이다.

<center>
<img src="https://github.com/user-attachments/assets/7c1a5dbb-e843-4963-b839-b2722fb4b53b" width="720" height=""/>
<p><b>[그림15]. MPD - Period </b></p>
</center>

period로 자른 컨텐츠를 제공하기 위해서 컨텐츠를 구성하고 있는 Component를 Adaptation Set이라 한다. 
ex. Video하나에 한국어, 영어를 제공하면(V1, A1-Kr, A2-Eng)총 3개의 Adaptation Set이 필요하다.

<center>
<img src="https://github.com/user-attachments/assets/4fc39d05-381b-4bbc-94af-90eb569d7134" width="720" height=""/>
<p><b>[그림16]. MPD - Adaptation </b></p>
</center>

나눠진 각각의 Component를 Qaulity에 맞게 나눈것. 예를 들면 Video와 Audio의 resolution을 Representation에 따라 상이하게 제공하는것이다.

<center>
<img src="https://github.com/user-attachments/assets/db457f4d-d508-49e2-a48b-b49634ce2636" width="720" height=""/>
<p><b>[그림17]. MPD - Representation </b></p>
</center>

Representation에서 각각의 Quailty마다 단위 시간을 둬서 Segment로 유지.(Period보다 더 작은 단위임.)
* MPD는 XML을 사용해서 코드로 만듦

<center>
<img src="https://github.com/user-attachments/assets/b6d44e88-09a3-484f-875f-ea986e18cb2f" width="720" height=""/>
<p><b>[그림18]. MPD - Segment </b></p>
</center>

영상을 클릭해서 시청하기전에 Client는 Server로 부터 아래와 같은 파일을 받음. MPD을 보면 Component를 2개 가지고 있고 Video와 Audio부분이 나눠짐을 알 수 있고 Adaaptation Set을 보면 720, 1280, 1920 3개의 Quailty를 가지고 있음을 알 수 있다.

<center>
<img src="https://github.com/user-attachments/assets/96a32f97-f6fa-4cd7-9865-defd0356a158" width="720" height=""/>
<p><b>[그림19]. MPD XML File </b></p>
</center>

정리
1. Period : Splicing of arbitrary content
2. Adaptation Set : Media Components(i.e. Video, audio, metadata, subtitle, etc..)
3. Representation : Different representation, the player can switch the quailty
4. Segment : Content chunks in temporal sequence

아래 그림을 보면 Adaptation Set안에 Representation block은 6개가 존재함을 알 수 있다.
언어도 설정할 수 있다면 Set또한 여러개가 존재함을 알 수 있다.

<center>
<img src="https://github.com/user-attachments/assets/d3061027-962d-4d36-82c1-ee1d4189e1c4" width="720" height=""/>
<p><b>[그림20]. 문제 예시 </b></p>
</center>

# 3. Latency lssue of ABS

Live Source T초에 사건이 발생했을때 Viewr가 T+@만큼시간에서 해당 사건을 관측할 수 있을때 @만큼 지연시간이 발생했다고 말한다.

* 보통 BroadBand에서는 20초 정도 지연시간을 가짐.

<center>
<img src="https://github.com/user-attachments/assets/1789c3fb-dae5-4f27-8e9e-407876e7216b" width="720" height=""/>
<p><b>[그림21]. 지연시간 개념 </b></p>
</center>

지연시간이 아래 서비스에서 치명적으로 작용하기 때문에 해결하기 위해 많은 방법을 사용하고 있다. 

<center>
<img src="https://github.com/user-attachments/assets/aa755b45-5e3a-45ac-8a7f-0508a3508b7b" width="720" height=""/>
<p><b>[그림22]. 여러 상황에서 지연시간 </b></p>
</center>

시간이 지남에 따라 Client는 더 좋은 화질의 영상을 보고 싶어하기 때문에 Encoding을 할때 더 많은 지연시간이 발생한다. 또한 Representation Set마다 Encoding해줘야하기 때문에 더 걸리며 Containing을 수행할때도 시간이 걸린다. 해당 과정에서 보통 2~10sec의 지연시간이 발생한다.

Content Delivery Network에 저장하는 시간도 3sec정도 걸린다. Content가 대용량일 경우 더 오래걸린다.

사용자 Device에서도 OS Version, 장치 성능, decoding, rendering에서도 충분히 발생해 10sec정도 걸린다.

<center>
<img src="https://github.com/user-attachments/assets/88b8b59e-334e-48f6-9656-66c5e21a183a" width="720" height=""/>
<p><b>[그림23]. 지연시간이 발생하는 이유 </b></p>
</center>

6초단위로 Segmentation Time를 설정하고 Live event가 T초에 발생했다고 하면 최소 Live event를 T+6초만큼 찍어야한다. T+6초부터 Encoding이 시작하게 되고 아무리 해당 과정이 빨라도 최소 6s가 걸린다. 추가로 CDN에 저장되어 Device에 도착하기까지 약 3s가 걸리므로 Encoding이 시작해서 내 Device에 도착할때까지 9s정도 걸린다. Live Event가 발생하고 Segmemntation Time 1구간이 Device에 도착하기까지 총 15s가 걸린다.

만약 Segmentation time를 3초로 설정하면 Encoding이 시작되는 시간까지도 줄어들고 Encoding시간도 줄어들고 CDN 서버에 저장하는 Content 용량도 적어지므로 Peroid 1구간이 Device에 도착하기까지의 시간이 줄어든다. -> 큰 Segmentation일 경우 더 많은 지연시간이 발생한다.

<center>
<img src="https://github.com/user-attachments/assets/87180287-34ac-40f3-a027-d7ebbdb0b255" width="720" height=""/>
<p><b>[그림24]. Live Encoder Phase </b></p>
</center>

<center>
<img src="https://github.com/user-attachments/assets/5b079705-4639-44ca-aaaf-7674352ad2d4" width="720" height=""/>
<p><b>[그림25]. Live Encoder Phase </b></p>
</center>

## 3.1. Latency Issue of ABS - MPEG DASH case

Content의 용량이 대용량일수록 더욱 더 많은 지연시간이 발생함.

<center>
<img src="https://github.com/user-attachments/assets/4fc7b3c1-00eb-4c6b-a837-85589cc04894" width="720" height=""/>
<p><b>[그림26].  </b></p>
</center>

비디오 스트리밍 서비스를 사용할때도 Chunk가 크면 Buffer의 크기도 커지므로 부담이 많이 가게된다.

<center>
<img src="https://github.com/user-attachments/assets/68d48e69-557c-40cb-bdff-44644ffe6d17" width="720" height=""/>
<p><b>[그림27]. MP4 Container </b></p>
</center>
